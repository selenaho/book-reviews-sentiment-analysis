{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Define and Solve an ML Problem of Your Choosing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab assignment, you will follow the machine learning life cycle and implement a model to solve a machine learning problem of your choosing. You will select a data set and choose a predictive problem that the data set supports.  You will then inspect the data with your problem in mind and begin to formulate a  project plan. You will then implement the machine learning project plan. \n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "1. Build Your DataFrame\n",
    "2. Define Your ML Problem\n",
    "3. Perform exploratory data analysis to understand your data.\n",
    "4. Define Your Project Plan\n",
    "5. Implement Your Project Plan:\n",
    "    * Prepare your data for your model.\n",
    "    * Fit your model to the training data and evaluate your model.\n",
    "    * Improve your model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Build Your DataFrame\n",
    "\n",
    "You will have the option to choose one of four data sets that you have worked with in this program:\n",
    "\n",
    "* The \"census\" data set that contains Census information from 1994: `censusData.csv`\n",
    "* Airbnb NYC \"listings\" data set: `airbnbListingsData.csv`\n",
    "* World Happiness Report (WHR) data set: `WHR2018Chapter2OnlineData.csv`\n",
    "* Book Review data set: `bookReviewsData.csv`\n",
    "\n",
    "Note that these are variations of the data sets that you have worked with in this program. For example, some do not include some of the preprocessing necessary for specific models. \n",
    "\n",
    "#### Load a Data Set and Save it as a Pandas DataFrame\n",
    "\n",
    "The code cell below contains filenames (path + filename) for each of the four data sets available to you.\n",
    "\n",
    "<b>Task:</b> In the code cell below, use the same method you have been using to load the data using `pd.read_csv()` and save it to DataFrame `df`. \n",
    "\n",
    "You can load each file as a new DataFrame to inspect the data before choosing your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This was perhaps the best of Johannes Steinhof...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This very fascinating book is a story written ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The four tales in this collection are beautifu...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The book contained more profanity than I expec...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We have now entered a second time of deep conc...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Positive Review\n",
       "0  This was perhaps the best of Johannes Steinhof...             True\n",
       "1  This very fascinating book is a story written ...             True\n",
       "2  The four tales in this collection are beautifu...             True\n",
       "3  The book contained more profanity than I expec...            False\n",
       "4  We have now entered a second time of deep conc...             True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File names of the four data sets\n",
    "adultDataSet_filename = os.path.join(os.getcwd(), \"data\", \"censusData.csv\")\n",
    "airbnbDataSet_filename = os.path.join(os.getcwd(), \"data\", \"airbnbListingsData.csv\")\n",
    "WHRDataSet_filename = os.path.join(os.getcwd(), \"data\", \"WHR2018Chapter2OnlineData.csv\")\n",
    "bookReviewDataSet_filename = os.path.join(os.getcwd(), \"data\", \"bookReviewsData.csv\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(bookReviewDataSet_filename)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Define Your ML Problem\n",
    "\n",
    "Next you will formulate your ML Problem. In the markdown cell below, answer the following questions:\n",
    "\n",
    "1. List the data set you have chosen.\n",
    "2. What will you be predicting? What is the label?\n",
    "3. Is this a supervised or unsupervised learning problem? Is this a clustering, classification or regression problem? Is it a binary classificaiton or multi-class classifiction problem?\n",
    "4. What are your features? (note: this list may change after your explore your data)\n",
    "5. Explain why this is an important problem. In other words, how would a company create value with a model that predicts this label?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set I chose is the book review data set. I'll be predicting whether or not a book review is a positive review or a negative review. The label is Positive Review, a column of booleans indicating whether the review is positive or not. This is a supervised learning problem because we are given labeled data to train the model. This is a classification problem because we're trying to predict a true/false value. This is binary classification. The feature is the Review column. A company would create value with a model that predicts this label by understanding how customers feel about certain products. This would allow them to understand their customers more which could help improve their chatbot or their marketing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Understand Your Data\n",
    "\n",
    "The next step is to perform exploratory data analysis. Inspect and analyze your data set with your machine learning problem in mind. Consider the following as you inspect your data:\n",
    "\n",
    "1. What data preparation techniques would you like to use? These data preparation techniques may include:\n",
    "\n",
    "    * addressing missingness, such as replacing missing values with means\n",
    "    * finding and replacing outliers\n",
    "    * renaming features and labels\n",
    "    * finding and replacing outliers\n",
    "    * performing feature engineering techniques such as one-hot encoding on categorical features\n",
    "    * selecting appropriate features and removing irrelevant features\n",
    "    * performing specific data cleaning and preprocessing techniques for an NLP problem\n",
    "    * addressing class imbalance in your data sample to promote fair AI\n",
    "    \n",
    "\n",
    "2. What machine learning model (or models) you would like to use that is suitable for your predictive problem and data?\n",
    "    * Are there other data preparation techniques that you will need to apply to build a balanced modeling data set for your problem and model? For example, will you need to scale your data?\n",
    " \n",
    " \n",
    "3. How will you evaluate and improve the model's performance?\n",
    "    * Are there specific evaluation metrics and methods that are appropriate for your model?\n",
    "    \n",
    "\n",
    "Think of the different techniques you have used to inspect and analyze your data in this course. These include using Pandas to apply data filters, using the Pandas `describe()` method to get insight into key statistics for each column, using the Pandas `dtypes` property to inspect the data type of each column, and using Matplotlib and Seaborn to detect outliers and visualize relationships between features and labels. If you are working on a classification problem, use techniques you have learned to determine if there is class imbalance.\n",
    "\n",
    "<b>Task</b>: Use the techniques you have learned in this course to inspect and analyze your data. You can import additional packages that you have used in this course that you will need to perform this task.\n",
    "\n",
    "<b>Note</b>: You can add code cells if needed by going to the <b>Insert</b> menu and clicking on <b>Insert Cell Below</b> in the drop-drown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review             False\n",
       "Positive Review    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking if any of the columns have null values\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Positive Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1973</td>\n",
       "      <td>1973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1865</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>I have read several of Hiaasen's books and lov...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Review Positive Review\n",
       "count                                                1973            1973\n",
       "unique                                               1865               2\n",
       "top     I have read several of Hiaasen's books and lov...           False\n",
       "freq                                                    3             993"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980\n",
      "993\n"
     ]
    }
   ],
   "source": [
    "#checking to see if there is a good balance between positive and negative reviews\n",
    "print((df['Positive Review']==True).sum())\n",
    "print((df['Positive Review']==False).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1973,)\n",
      "(1973,)\n"
     ]
    }
   ],
   "source": [
    "X = df['Review']\n",
    "y = df['Positive Review']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500     There is a reason this book has sold over 180,...\n",
       "1047    There is one thing that every cookbook author ...\n",
       "1667    Being an engineer in the aerospace industry I ...\n",
       "1646    I have no idea how this book has received the ...\n",
       "284     It is almost like dream comes true when I saw ...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.75, random_state=1234)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Define Your Project Plan\n",
    "\n",
    "Now that you understand your data, in the markdown cell below, define your plan to implement the remaining phases of the machine learning life cycle (data preparation, modeling, evaluation) to solve your ML problem. Answer the following questions:\n",
    "\n",
    "* Do you have a new feature list? If so, what are the features that you chose to keep and remove after inspecting the data?Â \n",
    "* Explain different data preparation techniques that you will use to prepare your data for modeling.\n",
    "* What is your model (or models)?\n",
    "* Describe your plan to train your model, analyze its performance and then improve the model. That is, describe your model building, validation and selection plan to produce a model that generalizes well to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set originally only had two columns: the review and the label column so those are the columns that I will be keeping for training the model. To prepare my data for modeling I will be using scikit learn's tf idf vectorizer which will handle the tokenization and pre-processing and convert the text to the numeric data matrix. I'm going to test what value of min_df leads to the best model performance. Min_df specifies when we ignore terms that have a document frequency strictly lower than the given threshold. I'm also going to test what value of ngram_range leads to the best model performance. This determines if having only unigrams, bigrams + unigrams, or only bigrams leads to a better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Implement Your Project Plan\n",
    "\n",
    "<b>Task:</b> In the code cell below, import additional packages that you have used in this course that you will need to implement your project plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Use the rest of this notebook to carry out your project plan. \n",
    "\n",
    "You will:\n",
    "\n",
    "1. Prepare your data for your model.\n",
    "2. Fit your model to the training data and evaluate your model.\n",
    "3. Improve your model's performance by performing model selection and/or feature selection techniques to find best model for your problem.\n",
    "\n",
    "Add code cells below and populate the notebook with commentary, code, analyses, results, and figures as you see fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Vectorizing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 18558: \n",
      "[('there', 16673), ('is', 9043), ('reason', 13533), ('this', 16714), ('book', 2189), ('has', 7803), ('sold', 15423), ('over', 11793), ('180', 73), ('000', 1), ('copies', 3867), ('it', 9076), ('gets', 7240), ('right', 14207), ('to', 16835), ('the', 16627), ('point', 12568), ('accompanies', 444), ('each', 5372), ('strategy', 15943), ('with', 18277), ('visual', 17844), ('aid', 750), ('so', 15386), ('you', 18497), ('can', 2604), ('get', 7239), ('mental', 10534), ('picture', 12402), ('in', 8491), ('your', 18501), ('head', 7844), ('further', 7051), ('its', 9088), ('section', 14743), ('on', 11601), ('analyzing', 974), ('stocks', 15886), ('and', 984), ('commentary', 3384), ('state', 15782), ('of', 11543), ('financial', 6568), ('statements', 15786), ('market', 10286), ('are', 1220), ('money', 10863), ('if', 8336), ('just', 9282), ('starting', 15774)]\n",
      "\n",
      "[[0.         0.16185315 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.01923341 0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a TfidfVectorizer object\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 2. Fit the vectorizer to X_train\n",
    "tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "# 3. Print the first 50 items in the vocabulary\n",
    "print(\"Vocabulary size {0}: \".format(len(tfidf_vectorizer.vocabulary_)))\n",
    "print(str(list(tfidf_vectorizer.vocabulary_.items())[0:50])+'\\n')\n",
    "\n",
    "      \n",
    "# 4. Transform *both* the training and test data using the fitted vectorizer and its 'transform' attribute\n",
    "X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "# 5. Print the matrix\n",
    "print(X_train_tfidf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on the test data: 0.9147\n",
      "The size of the feature space: 18558\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('there', 16673), ('is', 9043), ('reason', 13533), ('this', 16714), ('book', 2189)]:\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a LogisticRegression model object, and fit a Logistic Regression model to the transformed training data\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 2. Make predictions on the transformed test data using the predict_proba() method and \n",
    "# save the values of the second column\n",
    "probability_predictions = model.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "# 3. Make predictions on the transformed test data using the predict() method \n",
    "class_label_predictions = model.predict(X_test_tfidf)\n",
    "\n",
    "# 4. Compute the Area Under the ROC curve (AUC) for the test data. Note that this time we are using one \n",
    "# function 'roc_auc_score()' to compute the auc rather than using both 'roc_curve()' and 'auc()' as we have \n",
    "# done in the past\n",
    "auc = roc_auc_score(y_test, probability_predictions)\n",
    "print('AUC on the test data: {:.4f}'.format(auc))\n",
    "\n",
    "# 5. Print out the size of the resulting feature space using the 'vocabulary_' attribute of the vectorizer\n",
    "len_feature_space = len(tfidf_vectorizer.vocabulary_)\n",
    "print('The size of the feature space: {0}'.format(len_feature_space))\n",
    "\n",
    "# 6. Get a glimpse of the features:\n",
    "first_five = list(tfidf_vectorizer.vocabulary_.items())[0:5]\n",
    "print('Glimpse of first 5 entries of the mapping of a word to its column/feature index \\n{}:'.format(first_five))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review #1:\n",
      "\n",
      "I've been a fan of Carol Dweck's scholarly work for years. Her work on self-esteem, self-concept, and the incremental vs. entity theories of intelligence provides some of the most powerfully useful tools I've encountered for educators and parents in their work with children, as well as in their own self-awareness and lives. I'm delighted to see this information written here in such a user-friendly conversational tone, rich with stories that illustrate the nuances and complexities of Dweck's research and ideas. I'm recommending this book to all of my graduate students (teachers and principals working with gifted learners), as well as to parents of high-ability children.\n",
      "\n",
      "Dona Matthews, Ph.D., Director of the Hunter College Center for Gifted Studies and Education, City University of New York\n",
      "\n",
      "\n",
      "Prediction: Is this a good review? True\n",
      "\n",
      "Actual: Is this a good review? True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Review #1:\\n')\n",
    "print(X_test.to_numpy()[124])\n",
    "\n",
    "print('\\nPrediction: Is this a good review? {}\\n'.format(class_label_predictions[124])) \n",
    "\n",
    "print('Actual: Is this a good review? {}\\n'.format(y_test.to_numpy()[124]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min Document Frequency Value: 1\n",
      "AUC on the test data: 0.9268\n",
      "The size of the feature space: 138486\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('there', 119835), ('is', 61671), ('reason', 97323), ('this', 120815), ('book', 18054)]:\n",
      "Glimpse of first 5 stop words \n",
      "[]:\n",
      "\n",
      "Min Document Frequency Value: 10\n",
      "AUC on the test data: 0.9194\n",
      "The size of the feature space: 4023\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('there', 3352), ('is', 1687), ('reason', 2699), ('this', 3396), ('book', 464)]:\n",
      "Glimpse of first 5 stop words \n",
      "['better conveyed', 'sexism', 'author gary', 'national interest', 'tom']:\n",
      "\n",
      "Min Document Frequency Value: 100\n",
      "AUC on the test data: 0.8463\n",
      "The size of the feature space: 257\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('there', 199), ('is', 102), ('this', 207), ('book', 35), ('has', 81)]:\n",
      "Glimpse of first 5 stop words \n",
      "['better conveyed', 'sexism', 'author gary', 'national interest', 'tom']:\n",
      "\n",
      "Min Document Frequency Value: 1000\n",
      "AUC on the test data: 0.6436\n",
      "The size of the feature space: 9\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('is', 3), ('this', 7), ('book', 1), ('it', 4), ('to', 8)]:\n",
      "Glimpse of first 5 stop words \n",
      "['better conveyed', 'sexism', 'author gary', 'national interest', 'tom']:\n"
     ]
    }
   ],
   "source": [
    "# testing how min_df values affects model performance\n",
    "for min_df in [1,10,100,1000]:\n",
    "    \n",
    "    print('\\nMin Document Frequency Value: {0}'.format(min_df))\n",
    "    \n",
    "    # 1. Create a TfidfVectorizer object\n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=min_df, ngram_range=(1,2))\n",
    "\n",
    "    # 2. Fit the vectorizer to X_train\n",
    "    tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "    # 3. Transform the training and test data\n",
    "    X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    # 4. Create a LogisticRegression model object, and fit a Logistic Regression model to the transformed \n",
    "    # training data\n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # 5. Make predictions on the transformed test data using the predict_proba() method and save \n",
    "    # the values of the second column\n",
    "    probability_predictions = model.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "    # 6. Compute the Area Under the ROC curve (AUC) for the test data.\n",
    "    auc = roc_auc_score(y_test, probability_predictions)\n",
    "    print('AUC on the test data: {:.4f}'.format(auc))\n",
    "\n",
    "    # 7. Compute the size of the resulting feature space using the 'vocabulary_' attribute of the vectorizer\n",
    "    len_feature_space = len(tfidf_vectorizer.vocabulary_)\n",
    "    print('The size of the feature space: {0}'.format(len_feature_space))\n",
    "    \n",
    "    # 8. Get a glimpse of the features:\n",
    "    first_five = list(tfidf_vectorizer.vocabulary_.items())[0:5]\n",
    "    print('Glimpse of first 5 entries of the mapping of a word to its column/feature index \\n{}:'.format(first_five))\n",
    "\n",
    "    # 9: Print the first five \"stop words\" - words that we are ignoring\n",
    "    first_five_stop = list(tfidf_vectorizer.stop_words_)[0:5]\n",
    "    print('Glimpse of first 5 stop words \\n{}:'.format(first_five_stop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above results, the minimum document frequency value with the best performance is 1 because it leads to AUC of 0.9268. However, because the size of the feature space is 138486, this could slow down the model. With a min document frequency value of 10, this still leads to a pretty good AUC on the test data of 0.9194 with a significantly smaller size of feature space of 4023 so it won't be as slow as the model with the min document frequency value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ngram_range value: (1, 1)\n",
      "AUC on the test data: 0.9147\n",
      "The size of the feature space: 18558\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('there', 16673), ('is', 9043), ('reason', 13533), ('this', 16714), ('book', 2189)]:\n",
      "\n",
      "ngram_range value: (1, 2)\n",
      "AUC on the test data: 0.9268\n",
      "The size of the feature space: 138486\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('there', 119835), ('is', 61671), ('reason', 97323), ('this', 120815), ('book', 18054)]:\n",
      "\n",
      "ngram_range value: (2, 2)\n",
      "AUC on the test data: 0.9141\n",
      "The size of the feature space: 119928\n",
      "Glimpse of first 5 entries of the mapping of a word to its column/feature index \n",
      "[('there is', 103217), ('is reason', 53489), ('reason this', 83822), ('this book', 104169), ('book has', 16111)]:\n"
     ]
    }
   ],
   "source": [
    "#testing how ngram_range values affect the model's performance\n",
    "for r in [(1,1), (1,2), (2,2)]:\n",
    "    print('\\nngram_range value: {0}'.format(r))\n",
    "    \n",
    "    # 1. Create a TfidfVectorizer object\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=r)\n",
    "\n",
    "    # 2. Fit the vectorizer to X_train\n",
    "    tfidf_vectorizer.fit(X_train)\n",
    "\n",
    "    # 3. Transform the training and test data\n",
    "    X_train_tfidf = tfidf_vectorizer.transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    # 4. Create a LogisticRegression model object, and fit a Logistic Regression model to the transformed \n",
    "    # training data\n",
    "    model = LogisticRegression(max_iter=200)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # 5. Make predictions on the transformed test data using the predict_proba() method and save \n",
    "    # the values of the second column\n",
    "    probability_predictions = model.predict_proba(X_test_tfidf)[:,1]\n",
    "\n",
    "    # 6. Compute the Area Under the ROC curve (AUC) for the test data.\n",
    "    auc = roc_auc_score(y_test, probability_predictions)\n",
    "    print('AUC on the test data: {:.4f}'.format(auc))\n",
    "\n",
    "    # 7. Compute the size of the resulting feature space using the 'vocabulary_' attribute of the vectorizer\n",
    "    len_feature_space = len(tfidf_vectorizer.vocabulary_)\n",
    "    print('The size of the feature space: {0}'.format(len_feature_space))\n",
    "    \n",
    "    # 8. Get a glimpse of the features:\n",
    "    first_five = list(tfidf_vectorizer.vocabulary_.items())[0:5]\n",
    "    print('Glimpse of first 5 entries of the mapping of a word to its column/feature index \\n{}:'.format(first_five))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC on the test data didn't change much between the different ngram_range values. AUC of 0.9147, 0.9268, and 0.9141 isn't a dramatic difference. However, the size of the feature space differs pretty drastically between the different ngram_range values. So, if speed is important, then an ngram_range value of (1,1) would probably be the best option because it has a similar AUC to the other two options while having a feature space size of 18558 which is significantly less than the other two options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with help from Transforming Text Into Features for Sentiment Analysis code demo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
